{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 318 entries, 0 to 317\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   fecha_ingreso_sistema  318 non-null    datetime64[ns]\n",
      " 1   fecha                  318 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2)\n",
      "memory usage: 5.1 KB\n",
      "None\n",
      "  fecha_ingreso_sistema      fecha\n",
      "0            2023-10-30 2023-10-30\n",
      "1            2023-10-30 2023-10-30\n",
      "2            2023-10-27 2023-10-27\n",
      "3            2023-10-27 2023-10-27\n",
      "4            2023-10-27 2023-10-27\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "df_leads = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/prospectos_2023.csv\")\n",
    "\n",
    "# Define columns and their target data types\n",
    "columns_dtype = {\n",
    "    \"mes\": \"int64\",\n",
    "    \"partida_arancelaria\": \"int64\",\n",
    "    \"ruc_importador\": \"int64\",\n",
    "    \"dau\": \"int64\",\n",
    "    \"advalorem\": \"float64\",\n",
    "    \"unidades\": \"float64\",\n",
    "    \"peso_neto\": \"float64\",\n",
    "    \"peso_bruto\": \"float64\",\n",
    "    \"us$_fob\": \"float64\",\n",
    "    \"us$_flete\": \"float64\",\n",
    "    \"us$_seguro\": \"float64\",\n",
    "    \"us$_cif\": \"float64\",\n",
    "    \"bultos\": \"float64\",\n",
    "}\n",
    "\n",
    "# Filter out columns that don't exist in the dataset before applying dtype conversion\n",
    "existing_columns = {col: dtype for col, dtype in columns_dtype.items() if col in df_leads.columns}\n",
    "\n",
    "# Convert the existing columns to the appropriate data types\n",
    "df_leads = df_leads.astype(existing_columns)\n",
    "\n",
    "# List of date columns\n",
    "date_columns = [\"fecha_de_embarque\", \"fecha_de_llegada\", \"fecha_ingreso_sistema\", \"fecha\"]\n",
    "\n",
    "def preprocess_dates(column):\n",
    "    \"\"\"Preprocess the date columns by stripping spaces and replacing underscores.\"\"\"\n",
    "    return (\n",
    "        column.astype(str)\n",
    "        .str.strip()  # Remove leading/trailing spaces\n",
    "        .str.replace(\"_\", \" \")  # Replace underscores with spaces\n",
    "        .str.split(\" \", n=1, expand=True)[0]  # Keep only the date part before the time\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to date columns if they exist in the dataset\n",
    "for col in date_columns:\n",
    "    if col in df_leads.columns:\n",
    "        df_leads[col] = preprocess_dates(df_leads[col])\n",
    "\n",
    "# Convert to datetime format for the date columns\n",
    "for col in date_columns:\n",
    "    if col in df_leads.columns:\n",
    "        df_leads[col] = pd.to_datetime(df_leads[col], errors=\"coerce\")\n",
    "\n",
    "# Handle the case where some date columns might not be present\n",
    "# Only attempt to print if the columns are available\n",
    "existing_date_columns = [col for col in date_columns if col in df_leads.columns]\n",
    "\n",
    "# Print information and preview the data for available date columns\n",
    "if existing_date_columns:\n",
    "    print(df_leads[existing_date_columns].info())\n",
    "    print(df_leads[existing_date_columns].head())\n",
    "else:\n",
    "    print(\"No date columns available for preview.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fecha  dia  mes   año       regimen_aduanero              refrendo  \\\n",
      "0 2023-10-30   30   10  2023    importacion_courier  019-2023-91-01835123   \n",
      "1 2023-10-30   30   10  2023  importacion_a_consumo  028-2023-10-01841170   \n",
      "2 2023-10-27   27   10  2023  importacion_a_consumo  028-2023-10-01812140   \n",
      "3 2023-10-27   27   10  2023  importacion_a_consumo  028-2023-10-01815066   \n",
      "4 2023-10-27   27   10  2023  importacion_a_consumo  028-2023-10-01815066   \n",
      "\n",
      "   item  ruc_importador                                probable_importador  \\\n",
      "0     1    992833831001                                     agenkacom_s.a.   \n",
      "1    84    990792658001            importadora_lino_gamboa_cia._ltda._ilga   \n",
      "2     2    993252182001  import_green_power_technology,_equipment_&_mac...   \n",
      "3     9    992633980001                                         nadeu_s.a.   \n",
      "4    10    992633980001                                         nadeu_s.a.   \n",
      "\n",
      "  país_de_origen  ...                            dirección_consignatario  \\\n",
      "0          china  ...          av._de_las_americas_s/n_y_eugenio_almazan   \n",
      "1          china  ...  cdla_adace_calle_b_#_415_y_calle_novena(esq).f...   \n",
      "2          china  ...  guayas__samborondon__la_puntilla_satelite__fer...   \n",
      "3          china  ...  km_14_1/2_via_a_daule_guayaquil,ecuador_r.u.c....   \n",
      "4          china  ...  km_14_1/2_via_a_daule_guayaquil,ecuador_r.u.c....   \n",
      "\n",
      "  fecha_ingreso_sistema                                    caracteristicas  \\\n",
      "0            2023-10-30                                        desconocido   \n",
      "1            2023-10-30               cruceta_sola_p-acople_aireador_il-16   \n",
      "2            2023-10-27             spare_parts_for_acquaculture_equipment   \n",
      "3            2023-10-27  llave_p_coc_individual_p_pared_c_aireador_rose...   \n",
      "4            2023-10-27  llave_p_coc_individual_p_pared_c_aireador_rose...   \n",
      "\n",
      "               producto  marca_comercial año_fabricación modelo_mercaderia  \\\n",
      "0              generico         generico               0          generico   \n",
      "1  repuesto_de_aireador         generico            2023          il-16-30   \n",
      "2  partes_de_aireadores         generico            2023          generico   \n",
      "3                 llave         aquarame               0         e3030grey   \n",
      "4                 llave         aquarame               0         e3030blue   \n",
      "\n",
      "  us$_fob_unit                                 agencia_de_carga  código_tnan  \n",
      "0         0.00                         dhl_express_ecuador_s.a.            0  \n",
      "1         9.68  servicios_de_carga_internacional_s.a._secarinsa            0  \n",
      "2        27.89                                    farletza_s.a.            0  \n",
      "3         1.20                    zz_otros_liberadores_directos            0  \n",
      "4         1.20                    zz_otros_liberadores_directos            0  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "         fecha  dia  mes   año       regimen_aduanero              refrendo  \\\n",
      "313 2023-01-04    4    1  2023  importacion_a_consumo  028-2023-10-00014640   \n",
      "314 2023-01-04    4    1  2023  importacion_a_consumo  028-2023-10-00014640   \n",
      "315 2023-01-04    4    1  2023  importacion_a_consumo  028-2023-10-00014640   \n",
      "316 2023-01-03    3    1  2023  importacion_a_consumo  028-2023-10-00005673   \n",
      "317 2023-01-02    2    1  2023    importacion_courier  019-2023-91-00004531   \n",
      "\n",
      "     item  ruc_importador                            probable_importador  \\\n",
      "313    19    791724260001  crustaceos_&_peces_de_sud_america_s_a_crupesa   \n",
      "314    14    791724260001  crustaceos_&_peces_de_sud_america_s_a_crupesa   \n",
      "315    21    791724260001  crustaceos_&_peces_de_sud_america_s_a_crupesa   \n",
      "316     1    992442271001                                    hidmol_s.a.   \n",
      "317     1       908326366        castilla_fassio_maria_isabel_del_carmen   \n",
      "\n",
      "    país_de_origen  ...                            dirección_consignatario  \\\n",
      "313          china  ...                       25_de_junio_106_y_esmeraldas   \n",
      "314          china  ...                       25_de_junio_106_y_esmeraldas   \n",
      "315          china  ...                       25_de_junio_106_y_esmeraldas   \n",
      "316          china  ...  cedros_122_y_todos_los_santos_(urdesa_central)...   \n",
      "317  united_states  ...        av_de_las_amaericas_900_y_alejandro_andrade   \n",
      "\n",
      "    fecha_ingreso_sistema                             caracteristicas  \\\n",
      "313            2023-01-04           boya_de_aireador_electrico_sulong   \n",
      "314            2023-01-04                rueda_aireador_13_pltas_fudu   \n",
      "315            2023-01-04  cajas_de_transmision_de_aireador_electrico   \n",
      "316            2023-01-03                                    aireador   \n",
      "317            2023-01-02                                 desconocido   \n",
      "\n",
      "                                       producto  marca_comercial  \\\n",
      "313           boya_de_aireador_electrico_sulong         generico   \n",
      "314                rueda_aireador_13_pltas_fudu         generico   \n",
      "315  cajas_de_transmision_de_aireador_electrico         generico   \n",
      "316                                    aireador            annex   \n",
      "317                                    generico         generico   \n",
      "\n",
      "    año_fabricación modelo_mercaderia us$_fob_unit  \\\n",
      "313            2022          generico        20.00   \n",
      "314            2022          generico         5.00   \n",
      "315            2022          generico        75.00   \n",
      "316               0          generico       535.00   \n",
      "317               0          generico        79.19   \n",
      "\n",
      "                                      agencia_de_carga  código_tnan  \n",
      "313  mediterranean_shipping_company_del_ecuador_com...            0  \n",
      "314  mediterranean_shipping_company_del_ecuador_com...            0  \n",
      "315  mediterranean_shipping_company_del_ecuador_com...            0  \n",
      "316            cosco_shipping_lines_ecuador_cia._ltda.            0  \n",
      "317                            in_express-courier_s.a.            0  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "fecha                             datetime64[ns]\n",
      "dia                                        int64\n",
      "mes                                        int64\n",
      "año                                        int64\n",
      "regimen_aduanero                          object\n",
      "refrendo                                  object\n",
      "item                                       int64\n",
      "ruc_importador                             int64\n",
      "probable_importador                       object\n",
      "país_de_origen                            object\n",
      "pais_de_procedencia                       object\n",
      "ciudad_embarque                           object\n",
      "via_de_transporte                         object\n",
      "aduana                                    object\n",
      "partida_arancelaria                        int64\n",
      "descripción_arancelaria                   object\n",
      "descripcion_producto_comercial            object\n",
      "marca                                     object\n",
      "estado_de_mercancia                       object\n",
      "cantidad                                 float64\n",
      "unidad_de_medida                          object\n",
      "advalorem                                float64\n",
      "us$_fob                                  float64\n",
      "us$_flete                                float64\n",
      "us$_seguro                               float64\n",
      "us$_cif                                  float64\n",
      "embarcador                                object\n",
      "empresa_de_transporte                     object\n",
      "país_de_embarque                          object\n",
      "contenedor                                 int64\n",
      "agente_de_aduana                          object\n",
      "nave                                      object\n",
      "conocimiento_embarque                     object\n",
      "peso_neto_kg                             float64\n",
      "tipo_aforo                                object\n",
      "dirección_consignatario                   object\n",
      "fecha_ingreso_sistema             datetime64[ns]\n",
      "caracteristicas                           object\n",
      "producto                                  object\n",
      "marca_comercial                           object\n",
      "año_fabricación                            int64\n",
      "modelo_mercaderia                         object\n",
      "us$_fob_unit                             float64\n",
      "agencia_de_carga                          object\n",
      "código_tnan                                int64\n",
      "dtype: object\n",
      "fecha                             0\n",
      "dia                               0\n",
      "mes                               0\n",
      "año                               0\n",
      "regimen_aduanero                  0\n",
      "refrendo                          0\n",
      "item                              0\n",
      "ruc_importador                    0\n",
      "probable_importador               0\n",
      "país_de_origen                    0\n",
      "pais_de_procedencia               0\n",
      "ciudad_embarque                   0\n",
      "via_de_transporte                 0\n",
      "aduana                            0\n",
      "partida_arancelaria               0\n",
      "descripción_arancelaria           0\n",
      "descripcion_producto_comercial    0\n",
      "marca                             0\n",
      "estado_de_mercancia               0\n",
      "cantidad                          0\n",
      "unidad_de_medida                  0\n",
      "advalorem                         0\n",
      "us$_fob                           0\n",
      "us$_flete                         0\n",
      "us$_seguro                        0\n",
      "us$_cif                           0\n",
      "embarcador                        0\n",
      "empresa_de_transporte             0\n",
      "país_de_embarque                  0\n",
      "contenedor                        0\n",
      "agente_de_aduana                  0\n",
      "nave                              0\n",
      "conocimiento_embarque             0\n",
      "peso_neto_kg                      0\n",
      "tipo_aforo                        0\n",
      "dirección_consignatario           0\n",
      "fecha_ingreso_sistema             0\n",
      "caracteristicas                   0\n",
      "producto                          0\n",
      "marca_comercial                   0\n",
      "año_fabricación                   0\n",
      "modelo_mercaderia                 0\n",
      "us$_fob_unit                      0\n",
      "agencia_de_carga                  0\n",
      "código_tnan                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify the loaded data\n",
    "print(df_leads.head())\n",
    "print(df_leads.tail())\n",
    "print(df_leads.dtypes)\n",
    "print(df_leads.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fecha       regimen_aduanero              refrendo  item  \\\n",
      "0 2023-10-30    importacion_courier  019-2023-91-01835123     1   \n",
      "1 2023-10-30  importacion_a_consumo  028-2023-10-01841170    84   \n",
      "2 2023-10-27  importacion_a_consumo  028-2023-10-01812140     2   \n",
      "3 2023-10-27  importacion_a_consumo  028-2023-10-01815066     9   \n",
      "4 2023-10-27  importacion_a_consumo  028-2023-10-01815066    10   \n",
      "\n",
      "   ruc_importador                                probable_importador  \\\n",
      "0    992833831001                                     agenkacom_s.a.   \n",
      "1    990792658001            importadora_lino_gamboa_cia._ltda._ilga   \n",
      "2    993252182001  import_green_power_technology,_equipment_&_mac...   \n",
      "3    992633980001                                         nadeu_s.a.   \n",
      "4    992633980001                                         nadeu_s.a.   \n",
      "\n",
      "  país_de_origen pais_de_procedencia ciudad_embarque via_de_transporte  ...  \\\n",
      "0          china               china     panama_city             aerea  ...   \n",
      "1          china               china        shanghai          maritimo  ...   \n",
      "2          china               china          ningbo          maritimo  ...   \n",
      "3          china               china        shanghai          maritimo  ...   \n",
      "4          china               china        shanghai          maritimo  ...   \n",
      "\n",
      "                     tipo_aforo  \\\n",
      "0  aforo-aforo_fisico_intrusivo   \n",
      "1        aforo-aforo_automatico   \n",
      "2        aforo-aforo_automatico   \n",
      "3        aforo-aforo_automatico   \n",
      "4        aforo-aforo_automatico   \n",
      "\n",
      "                             dirección_consignatario  \\\n",
      "0          av._de_las_americas_s/n_y_eugenio_almazan   \n",
      "1  cdla_adace_calle_b_#_415_y_calle_novena(esq).f...   \n",
      "2  guayas__samborondon__la_puntilla_satelite__fer...   \n",
      "3  km_14_1/2_via_a_daule_guayaquil,ecuador_r.u.c....   \n",
      "4  km_14_1/2_via_a_daule_guayaquil,ecuador_r.u.c....   \n",
      "\n",
      "                                     caracteristicas              producto  \\\n",
      "0                                        desconocido              generico   \n",
      "1               cruceta_sola_p-acople_aireador_il-16  repuesto_de_aireador   \n",
      "2             spare_parts_for_acquaculture_equipment  partes_de_aireadores   \n",
      "3  llave_p_coc_individual_p_pared_c_aireador_rose...                 llave   \n",
      "4  llave_p_coc_individual_p_pared_c_aireador_rose...                 llave   \n",
      "\n",
      "  marca_comercial año_fabricación  modelo_mercaderia us$_fob_unit  \\\n",
      "0        generico               0           generico         0.00   \n",
      "1        generico            2023           il-16-30         9.68   \n",
      "2        generico            2023           generico        27.89   \n",
      "3        aquarame               0          e3030grey         1.20   \n",
      "4        aquarame               0          e3030blue         1.20   \n",
      "\n",
      "                                  agencia_de_carga  código_tnan  \n",
      "0                         dhl_express_ecuador_s.a.            0  \n",
      "1  servicios_de_carga_internacional_s.a._secarinsa            0  \n",
      "2                                    farletza_s.a.            0  \n",
      "3                    zz_otros_liberadores_directos            0  \n",
      "4                    zz_otros_liberadores_directos            0  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "(318, 41)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check if 'fecha' is already present and 'fecha_ingreso_sistema' exists\n",
    "if 'fecha' not in df_leads.columns and 'fecha_ingreso_sistema' in df_leads.columns:\n",
    "    # Rename 'fecha_ingreso_sistema' to 'fecha' and move it to the first position\n",
    "    df_leads = df_leads.rename(columns={'fecha_ingreso_sistema': 'fecha'})\n",
    "    # Reorder columns so that 'fecha' is the first column\n",
    "    columns_order = ['fecha'] + [col for col in df_leads.columns if col != 'fecha']\n",
    "    df_leads = df_leads[columns_order]\n",
    "else:\n",
    "    # If 'fecha' already exists, just drop 'fecha_ingreso_sistema' and move on\n",
    "    if 'fecha_ingreso_sistema' in df_leads.columns:\n",
    "        df_leads = df_leads.drop(columns='fecha_ingreso_sistema')\n",
    "\n",
    "# Step 2: Drop the other date columns and 'mes', but only if they exist in the dataset\n",
    "columns_to_drop = ['fecha_de_embarque', 'fecha_de_llegada', 'mes', 'dia', 'año']\n",
    "columns_to_drop_existing = [col for col in columns_to_drop if col in df_leads.columns]\n",
    "\n",
    "# Remove columns that exist in the dataset\n",
    "df_leads = df_leads.drop(columns=columns_to_drop_existing)\n",
    "\n",
    "# Step 3: Check the updated dataset\n",
    "print(df_leads.head())\n",
    "print(df_leads.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "df_leads.to_csv('/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2023.csv', index=False)\n",
    "\n",
    "print(\"File saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for 2017:\n",
      "Shape: (291, 39)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'pais_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_de_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan']\n",
      "\n",
      "Dataset for 2018:\n",
      "Shape: (318, 39)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'pais_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan']\n",
      "\n",
      "Dataset for 2019:\n",
      "Shape: (246, 39)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'pais_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan']\n",
      "\n",
      "Dataset for 2020:\n",
      "Shape: (202, 39)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'pais_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan']\n",
      "\n",
      "Dataset for 2021:\n",
      "Shape: (381, 39)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'pais_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan']\n",
      "\n",
      "Dataset for 2022:\n",
      "Shape: (485, 39)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'país_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripción_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'país_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan']\n",
      "\n",
      "Dataset for 2023:\n",
      "Shape: (318, 39)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'país_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripción_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'país_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "shape_2017 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2017.csv\")\n",
    "shape_2018 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2018.csv\")\n",
    "shape_2019 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2019.csv\")\n",
    "shape_2020 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2020.csv\")\n",
    "shape_2021 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2021.csv\")\n",
    "shape_2022 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2022.csv\")\n",
    "shape_2023 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/concat_2023.csv\")\n",
    "\n",
    "# List of DataFrames and their names for easier iteration\n",
    "datasets = [\n",
    "    (\"2017\", shape_2017),\n",
    "    (\"2018\", shape_2018),\n",
    "    (\"2019\", shape_2019),\n",
    "    (\"2020\", shape_2020),\n",
    "    (\"2021\", shape_2021),\n",
    "    (\"2022\", shape_2022),\n",
    "    (\"2023\", shape_2023)\n",
    "]\n",
    "\n",
    "# Columns to drop for years 2018-2023\n",
    "columns_to_drop = ['agencia_de_carga', 'contenedor']\n",
    "\n",
    "# Iterate through datasets to drop columns for 2018-2023 and print their shape and columns\n",
    "for year, df in datasets:\n",
    "    if year in [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]:\n",
    "        # Drop columns if they exist in the dataset\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(f\"Dataset for {year}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to /home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2017-2023.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'fecha' is a datetime column in all datasets for proper merging and sorting\n",
    "for year, df in datasets:\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "# Merge the datasets on 'fecha'\n",
    "df_merged = pd.concat([df for year, df in datasets], axis=0, ignore_index=True)\n",
    "\n",
    "# Sort the merged dataset by 'fecha' from oldest to newest\n",
    "df_merged = df_merged.sort_values(by='fecha').reset_index(drop=True)\n",
    "\n",
    "# Save the merged and sorted dataset to a new CSV file\n",
    "output_path = \"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2017-2023.csv\"\n",
    "df_merged.to_csv(output_path, index=False)\n",
    "\n",
    "# Confirm successful save\n",
    "print(f\"Merged file saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'verifcador' not found.\n",
      "(173, 35)\n"
     ]
    }
   ],
   "source": [
    "df_adjust = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2010-2012.csv\")\n",
    "\n",
    "if 'verifcador' in df_adjust.columns:\n",
    "    df_adjust = df_adjust.drop(columns=\"verificador\")\n",
    "else:\n",
    "    print(\"Column 'verifcador' not found.\")\n",
    "\n",
    "output_path = \"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2010-2012.csv\"\n",
    "df_adjust.to_csv(output_path, index=False)\n",
    "\n",
    "print(df_adjust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for 2010-2012:\n",
      "Shape: (173, 35)\n",
      "Columns: ['fecha', 'partida_arancelaria', 'producto', 'descripcion_arancelaria', 'ruc_importador', 'probable_importador', 'direccion', 'telefono', 'email1', 'refrendo', 'regimen_aduanero', 'pais_de_origen', 'pais_de_embarque', 'puerto_de_embarque', 'via_de_transporte', 'aduana', 'nave', 'empresa_de_transporte', 'agencia_de_transporte', 'dau', 'advalorem', 'unidades', 'peso_neto', 'peso_bruto', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'bultos', 'tipo_de_bulto', 'estado_de_mercancia', 'embarcador', 'tipo_aforo', 'verificador', 'agente_de_aduana']\n",
      "\n",
      "Dataset for 2013-2016:\n",
      "Shape: (787, 34)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'dau', 'item', 'ruc_importador', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'puerto_de_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'bultos', 'unidades', 'tipo_de_bulto', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'pais_de_embarque', 'container', 'agente_de_aduana', 'nave', 'numero_de_manifiesto', 'conocimiento_de_embarque', 'peso_neto']\n",
      "\n",
      "Dataset for 2017-2023:\n",
      "Shape: (2241, 45)\n",
      "Columns: ['fecha', 'regimen_aduanero', 'refrendo', 'item', 'ruc_importador', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripcion_producto_comercial', 'marca', 'estado_de_mercancia', 'cantidad', 'unidad_de_medida', 'advalorem', 'us$_fob', 'us$_flete', 'us$_seguro', 'us$_cif', 'embarcador', 'empresa_de_transporte', 'pais_de_embarque', 'agente_de_aduana', 'nave', 'conocimiento_de_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca_comercial', 'año_fabricación', 'modelo_mercaderia', 'us$_fob_unit', 'código_tnan', 'contenedor', 'conocimiento_embarque', 'agencia_de_carga', 'país_de_origen', 'descripción_arancelaria', 'país_de_embarque']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the narrowed down datasets\n",
    "df_2010_2012 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2010-2012.csv\")\n",
    "df_2013_2016 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2013-2016.csv\")\n",
    "df_2017_2023 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2017-2023.csv\")\n",
    "\n",
    "# Display the shape and columns of each dataset\n",
    "datasets_info = {\n",
    "    \"2010-2012\": (df_2010_2012.shape, df_2010_2012.columns.tolist()),\n",
    "    \"2013-2016\": (df_2013_2016.shape, df_2013_2016.columns.tolist()),\n",
    "    \"2017-2023\": (df_2017_2023.shape, df_2017_2023.columns.tolist())\n",
    "}\n",
    "\n",
    "# Print dataset information\n",
    "for year, (shape, columns) in datasets_info.items():\n",
    "    print(f\"Dataset for {year}:\")\n",
    "    print(f\"Shape: {shape}\")\n",
    "    print(f\"Columns: {columns}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present only in 2010-2012 and not in 2013-2016 or 2017-2023:\n",
      "{'telefono', 'verificador', 'agencia_de_transporte', 'peso_bruto', 'email1', 'direccion'}\n",
      "\n",
      "\n",
      "Columns present only in 2013-2016 and not in 2010-2012 or 2017-2023:\n",
      "{'numero_de_manifiesto', 'container'}\n",
      "\n",
      "\n",
      "Columns present only in 2017-2023 and not in 2010-2012 or 2013-2016:\n",
      "{'ciudad_embarque', 'caracteristicas', 'descripción_arancelaria', 'país_de_origen', 'agencia_de_carga', 'modelo_mercaderia', 'marca_comercial', 'país_de_embarque', 'año_fabricación', 'unidad_de_medida', 'contenedor', 'dirección_consignatario', 'peso_neto_kg', 'código_tnan', 'us$_fob_unit', 'cantidad', 'conocimiento_embarque'}\n"
     ]
    }
   ],
   "source": [
    "# Get the columns of each dataset\n",
    "cols_2010_2012 = set(df_2010_2012.columns)\n",
    "cols_2013_2016 = set(df_2013_2016.columns)\n",
    "cols_2017_2023 = set(df_2017_2023.columns)\n",
    "\n",
    "# Find columns present only in 2010-2012 and not in the other datasets\n",
    "only_2010_2012 = cols_2010_2012 - cols_2013_2016 - cols_2017_2023\n",
    "\n",
    "# Find columns present only in 2013-2016 and not in the other datasets\n",
    "only_2013_2016 = cols_2013_2016 - cols_2010_2012 - cols_2017_2023\n",
    "\n",
    "# Find columns present only in 2017-2023 and not in the other datasets\n",
    "only_2017_2023 = cols_2017_2023 - cols_2010_2012 - cols_2013_2016\n",
    "\n",
    "# Display the results\n",
    "print(\"Columns present only in 2010-2012 and not in 2013-2016 or 2017-2023:\")\n",
    "print(only_2010_2012)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Columns present only in 2013-2016 and not in 2010-2012 or 2017-2023:\")\n",
    "print(only_2013_2016)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Columns present only in 2017-2023 and not in 2010-2012 or 2013-2016:\")\n",
    "print(only_2017_2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present in all datasets (2010-2012, 2013-2016, 2017-2023):\n",
      "{'pais_de_embarque', 'fecha', 'descripcion_arancelaria', 'empresa_de_transporte', 'via_de_transporte', 'aduana', 'us$_fob', 'partida_arancelaria', 'pais_de_origen', 'refrendo', 'estado_de_mercancia', 'us$_flete', 'nave', 'probable_importador', 'us$_seguro', 'us$_cif', 'agente_de_aduana', 'embarcador', 'ruc_importador', 'advalorem', 'regimen_aduanero'}\n",
      "Columns present in both 2013-2016 and 2017-2023:\n",
      "{'pais_de_embarque', 'fecha', 'pais_de_procedencia', 'descripcion_arancelaria', 'empresa_de_transporte', 'via_de_transporte', 'aduana', 'us$_fob', 'partida_arancelaria', 'pais_de_origen', 'refrendo', 'item', 'marca', 'conocimiento_de_embarque', 'estado_de_mercancia', 'us$_flete', 'nave', 'probable_importador', 'us$_seguro', 'us$_cif', 'descripcion_producto_comercial', 'embarcador', 'agente_de_aduana', 'ruc_importador', 'advalorem', 'regimen_aduanero'}\n",
      "Columns present in both 2010-2012 and 2017-2023:\n",
      "{'pais_de_embarque', 'fecha', 'descripcion_arancelaria', 'empresa_de_transporte', 'via_de_transporte', 'aduana', 'us$_fob', 'partida_arancelaria', 'pais_de_origen', 'refrendo', 'estado_de_mercancia', 'us$_flete', 'nave', 'probable_importador', 'us$_seguro', 'us$_cif', 'agente_de_aduana', 'embarcador', 'ruc_importador', 'tipo_aforo', 'advalorem', 'regimen_aduanero', 'producto'}\n",
      "Columns present in both 2010-2012 and 2013-2016:\n",
      "{'pais_de_embarque', 'fecha', 'descripcion_arancelaria', 'empresa_de_transporte', 'via_de_transporte', 'puerto_de_embarque', 'aduana', 'us$_fob', 'partida_arancelaria', 'pais_de_origen', 'dau', 'refrendo', 'peso_neto', 'bultos', 'estado_de_mercancia', 'us$_flete', 'nave', 'probable_importador', 'us$_seguro', 'us$_cif', 'embarcador', 'agente_de_aduana', 'unidades', 'ruc_importador', 'tipo_de_bulto', 'advalorem', 'regimen_aduanero'}\n"
     ]
    }
   ],
   "source": [
    "# Find columns present in all three datasets\n",
    "common_all_datasets = cols_2010_2012 & cols_2013_2016 & cols_2017_2023\n",
    "\n",
    "# Display common columns across all datasets\n",
    "if common_all_datasets:\n",
    "    print(\"Columns present in all datasets (2010-2012, 2013-2016, 2017-2023):\")\n",
    "    print(common_all_datasets)\n",
    "else:\n",
    "    print(\"No columns found in all datasets.\\n\")\n",
    "\n",
    "# Find columns present in 2013-2016 and not in 2017-2023\n",
    "common_2013_2016_2017_2023 = cols_2013_2016 & cols_2017_2023\n",
    "if common_2013_2016_2017_2023:\n",
    "    print(\"Columns present in both 2013-2016 and 2017-2023:\")\n",
    "    print(common_2013_2016_2017_2023)\n",
    "else:\n",
    "    print(\"No common columns found between 2013-2016 and 2017-2023.\\n\")\n",
    "\n",
    "# Find columns present in 2010-2012 and not in 2017-2023\n",
    "common_2010_2012_2017_2023 = cols_2010_2012 & cols_2017_2023\n",
    "if common_2010_2012_2017_2023:\n",
    "    print(\"Columns present in both 2010-2012 and 2017-2023:\")\n",
    "    print(common_2010_2012_2017_2023)\n",
    "else:\n",
    "    print(\"No common columns found between 2010-2012 and 2017-2023.\\n\")\n",
    "\n",
    "# Find columns present in 2010-2012 and not in 2013-2016\n",
    "common_2010_2012_2013_2016 = cols_2010_2012 & cols_2013_2016\n",
    "if common_2010_2012_2013_2016:\n",
    "    print(\"Columns present in both 2010-2012 and 2013-2016:\")\n",
    "    print(common_2010_2012_2013_2016)\n",
    "else:\n",
    "    print(\"No common columns found between 2010-2012 and 2013-2016.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to /home/luisvinatea/Data/Gdrive/BERAQUA/docs/datasets/imp_aireadores_2010-2023.csv\n",
      "Dropped data saved to /home/luisvinatea/Data/Gdrive/BERAQUA/docs/datasets/product_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load datasets\n",
    "df_2010_2012 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2010-2012.csv\")\n",
    "df_2013_2016 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2013-2016.csv\")\n",
    "df_2017_2023 = pd.read_csv(\"/home/luisvinatea/Data/Gdrive/BERAQUA/processed/prospectos_importadores/concat/merged/imp_aireadores_2017-2023.csv\")\n",
    "\n",
    "# List of columns to drop for each dataset\n",
    "columns_to_drop_2010_2012 = ['telefono', 'agencia_de_transporte', 'email1', 'direccion', 'descripcion_arancelaria', 'pais_de_origen', 'agencia_de_carga']\n",
    "columns_to_drop_2013_2016 = ['descripcion_arancelaria', 'pais_de_origen', 'agencia_de_carga']\n",
    "columns_to_drop_2017_2023 = ['descripcion_arancelaria', 'pais_de_origen', 'agencia_de_carga']\n",
    "\n",
    "# Rename columns as required\n",
    "df_2010_2012 = df_2010_2012.rename(columns={'email1': 'email'})\n",
    "\n",
    "# Initialize dictionary to store dropped column data\n",
    "dropped_data = {}\n",
    "\n",
    "# Create a helper function to store dropped columns in the dictionary\n",
    "def store_dropped_columns(df, year_range):\n",
    "    dropped_columns = {}\n",
    "    \n",
    "    # Check for each column before accessing\n",
    "    for column in ['telefono', 'agencia_de_transporte', 'email', 'direccion', 'descripcion_arancelaria', 'pais_de_origen', 'agencia_de_carga', 'direccion_consignatario', 'modelo_mercaderia', 'marca_comercial', 'us$_fob']:\n",
    "        if column in df.columns:\n",
    "            dropped_columns[column] = df[column]\n",
    "        else:\n",
    "            dropped_columns[column] = None\n",
    "    \n",
    "    # Group by 'probable_importador' and aggregate values\n",
    "    grouped_data = {}\n",
    "    for key, values in dropped_columns.items():\n",
    "        if values is not None:\n",
    "            grouped_data[key] = df.groupby('probable_importador')[key].apply(list).to_dict()\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "# Apply the function to each dataset\n",
    "dropped_data_2010_2012 = store_dropped_columns(df_2010_2012, \"2010-2012\")\n",
    "dropped_data_2013_2016 = store_dropped_columns(df_2013_2016, \"2013-2016\")\n",
    "dropped_data_2017_2023 = store_dropped_columns(df_2017_2023, \"2017-2023\")\n",
    "\n",
    "# Combine all dropped data into a single dictionary\n",
    "dropped_data.update(dropped_data_2010_2012)\n",
    "dropped_data.update(dropped_data_2013_2016)\n",
    "dropped_data.update(dropped_data_2017_2023)\n",
    "\n",
    "# Drop columns from datasets (checking if the column exists before dropping)\n",
    "df_2010_2012 = df_2010_2012.drop(columns=[col for col in columns_to_drop_2010_2012 if col in df_2010_2012.columns], errors='ignore')\n",
    "df_2013_2016 = df_2013_2016.drop(columns=[col for col in columns_to_drop_2013_2016 if col in df_2013_2016.columns], errors='ignore')\n",
    "df_2017_2023 = df_2017_2023.drop(columns=[col for col in columns_to_drop_2017_2023 if col in df_2017_2023.columns], errors='ignore')\n",
    "\n",
    "# Merge all datasets into one\n",
    "df_merged = pd.concat([df_2010_2012, df_2013_2016, df_2017_2023], axis=0, ignore_index=True)\n",
    "\n",
    "# Sort by 'fecha'\n",
    "df_merged['fecha'] = pd.to_datetime(df_merged['fecha'])\n",
    "df_merged = df_merged.sort_values(by='fecha').reset_index(drop=True)\n",
    "\n",
    "# Save the merged dataframe to CSV\n",
    "output_path = \"/home/luisvinatea/Data/Gdrive/BERAQUA/docs/datasets/imp_aireadores_2010-2023.csv\"\n",
    "df_merged.to_csv(output_path, index=False)\n",
    "\n",
    "# Save the dropped data to a JSON file\n",
    "dropped_data_path = \"/home/luisvinatea/Data/Gdrive/BERAQUA/docs/datasets/product_data.json\"\n",
    "with open(dropped_data_path, 'w') as json_file:\n",
    "    json.dump(dropped_data, json_file, indent=4)\n",
    "\n",
    "# Confirm successful save\n",
    "print(f\"Merged file saved to {output_path}\")\n",
    "print(f\"Dropped data saved to {dropped_data_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
