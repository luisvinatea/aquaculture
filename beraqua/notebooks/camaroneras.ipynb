{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for URL: https://www.google.com/search?q=industrial pesquera santa priscila s.a. | Switching search engine...\n",
      "Scraping completed and data saved to 'exportadores_updated.csv'\n",
      "Phone scraping completed and data saved to 'exportadores_updated.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Function to scrape the link\n",
    "def scrapelink(link):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(link, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if hasattr(response, 'status_code') and response.status_code == 429:\n",
    "            print(f\"Rate limit hit for URL: {link} | Switching search engine...\")\n",
    "            return 'rate_limit'\n",
    "        else:\n",
    "            print(f\"Error fetching the URL: {link} | Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract contact information from the soup\n",
    "def extract_contact_info(soup):\n",
    "    contact_info = {\n",
    "        'email': None,\n",
    "        'phone': None\n",
    "    }\n",
    "    \n",
    "    if soup is None or soup == 'rate_limit':\n",
    "        return contact_info\n",
    "    \n",
    "    try:\n",
    "        # Extract email addresses\n",
    "        email_regex = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}'\n",
    "        email_matches = re.findall(email_regex, soup.text)\n",
    "        if email_matches:\n",
    "            contact_info['email'] = email_matches[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting contact info: {e}\")\n",
    "        \n",
    "    return contact_info\n",
    "\n",
    "# Load your DataFrame\n",
    "df_exportadores = pd.read_csv('/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/docs/reports/agrupacion_exportadores.csv')  # Assuming you have a CSV file\n",
    "\n",
    "# Append new columns to store the scraped data if they don't already exist\n",
    "for column in ['email', 'phone']:\n",
    "    if column not in df_exportadores.columns:\n",
    "        df_exportadores[column] = None\n",
    "\n",
    "# Iterate over the keywords and scrape data in batches of 5 to fill email column first\n",
    "search_engines = [\n",
    "    'https://www.google.com/search?q={}',\n",
    "    'https://search.yahoo.com/search?p={}',\n",
    "    'https://www.bing.com/search?q={}',\n",
    "    'https://duckduckgo.com/?q={}']\n",
    "\n",
    "search_engine_status = {engine: True for engine in search_engines}\n",
    "\n",
    "for i in range(0, len(df_exportadores), 5):\n",
    "    batch = df_exportadores.iloc[i:i+5]\n",
    "    \n",
    "    for index, row in batch.iterrows():\n",
    "        if pd.notna(row['email']):\n",
    "            # Skip rows where email is already present\n",
    "            continue\n",
    "        \n",
    "        keyword = row['probable_exportador'].replace('_', ' ')\n",
    "        \n",
    "        try:\n",
    "            success = False\n",
    "            \n",
    "            for search_engine in search_engines:\n",
    "                if not search_engine_status[search_engine]:\n",
    "                    # Skip search engines that have hit the rate limit\n",
    "                    continue\n",
    "                \n",
    "                if success:\n",
    "                    break\n",
    "                \n",
    "                search_link = search_engine.format(keyword)\n",
    "                soup = scrapelink(search_link)\n",
    "                \n",
    "                if soup == 'rate_limit':\n",
    "                    search_engine_status[search_engine] = False\n",
    "                    continue\n",
    "                \n",
    "                if soup:\n",
    "                    contact_info = extract_contact_info(soup)\n",
    "                    if contact_info['email']:\n",
    "                        # Update DataFrame with the scraped email\n",
    "                        df_exportadores.at[index, 'email'] = contact_info['email']\n",
    "                        success = True\n",
    "                        \n",
    "            # Sleep to avoid being blocked by search engines\n",
    "            time.sleep(random.uniform(10, 15))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing keyword '{keyword}': {e}\")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_exportadores.to_csv('exportadores_updated.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed and data saved to 'exportadores_updated.csv'\")\n",
    "\n",
    "# Now use the filled dataset to search for phone numbers\n",
    "for index, row in df_exportadores.iterrows():\n",
    "    if pd.isna(row['email']) or pd.notna(row['phone']):\n",
    "        # Skip rows where email is missing or phone is already present\n",
    "        continue\n",
    "    \n",
    "    keyword = row['email']\n",
    "    \n",
    "    try:\n",
    "        success = False\n",
    "        \n",
    "        for search_engine in search_engines:\n",
    "            if not search_engine_status[search_engine]:\n",
    "                # Skip search engines that have hit the rate limit\n",
    "                continue\n",
    "            \n",
    "            if success:\n",
    "                break\n",
    "            \n",
    "            search_link = search_engine.format(keyword)\n",
    "            soup = scrapelink(search_link)\n",
    "            \n",
    "            if soup == 'rate_limit':\n",
    "                search_engine_status[search_engine] = False\n",
    "                continue\n",
    "            \n",
    "            if soup:\n",
    "                # Extract phone numbers (start with 9 and length of 9 digits)\n",
    "                phone_regex = r'\\+?593?\\s?\\(?\\d{2}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}'\n",
    "                phone_matches = re.findall(phone_regex, soup.text)\n",
    "                for phone in phone_matches:\n",
    "                    phone_numbers_only = re.sub(r'\\D', '', phone)\n",
    "                    if len(phone_numbers_only) == 9 and phone_numbers_only.startswith('9'):\n",
    "                        df_exportadores.at[index, 'phone'] = phone_numbers_only\n",
    "                        success = True\n",
    "                        break\n",
    "        \n",
    "        # Sleep to avoid being blocked by search engines\n",
    "        time.sleep(random.uniform(10, 15))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing keyword '{keyword}': {e}\")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_exportadores.to_csv('exportadores_updated.csv', index=False)\n",
    "\n",
    "print(\"Phone scraping completed and data saved to 'exportadores_updated.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 probable_exportador  valor_total_exportado  \\\n",
      "0            industrial_pesquera_santa_priscila_s.a.           8.459977e+09   \n",
      "1  operadora_y_procesadora_de_productos_marinos_o...           4.762433e+09   \n",
      "2                sociedad_nacional_de_galapagos_c.a.           3.024318e+09   \n",
      "3                expalsa_exportadora_de_alimentos_sa           2.203737e+09   \n",
      "4                                    promarisco_s.a.           1.613648e+09   \n",
      "\n",
      "   peso_total_mercancia  cantidad_total_unidades pais_mas_frecuente producto  \\\n",
      "0          9.120026e+08               1147696108              china  camaron   \n",
      "1          5.943874e+08                722805761              china  camaron   \n",
      "2          4.561799e+08                529295744      united_states  camaron   \n",
      "3          3.576516e+08                482192051              china  camaron   \n",
      "4          2.563640e+08                284210200              spain  camaron   \n",
      "\n",
      "  email  phone  \n",
      "0   NaN    NaN  \n",
      "1   NaN    NaN  \n",
      "2   NaN    NaN  \n",
      "3   NaN    NaN  \n",
      "4   NaN    NaN  \n",
      "(275, 8)\n",
      "phone    275\n",
      "email    261\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_scraped = pd.read_csv('/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/docs/reports/exportadores_updated.csv')\n",
    "\n",
    "# Trim whitespaces from the entire dataframe, including numeric columns\n",
    "df_scraped = df_scraped.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "print(df_scraped.head())\n",
    "print(df_scraped.shape)\n",
    "\n",
    "df_scraped.to_csv('/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/docs/reports/exportadores_updated.csv', index=False)\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values = df_scraped.isnull().sum()\n",
    "\n",
    "# Display the missing values in descending order\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "print(missing_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
