{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df_costs = pd.read_csv('/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/concat_2016.csv')\n",
    "\n",
    "df_costs['fecha'] = pd.to_datetime(df_costs['fecha'])\n",
    "\n",
    "def clean_data(df_costs):\n",
    "    try:\n",
    "        # Split text using string '_' in column: 'modelo_mercaderia'\n",
    "        if 'modelo_mercaderia' in df_costs.columns:\n",
    "            loc_0 = df_costs.columns.get_loc('modelo_mercaderia')\n",
    "            df_costs_split = df_costs['modelo_mercaderia'].str.split(pat='_', expand=True, n=1).add_prefix('modelo_mercaderia_')\n",
    "            \n",
    "            # Concatenate the new columns and the original dataframe, dropping 'modelo_mercaderia'\n",
    "            df_costs = pd.concat([df_costs.iloc[:, :loc_0], df_costs_split, df_costs.iloc[:, loc_0 + 1:]], axis=1)\n",
    "\n",
    "            # Replace non-missing entries in modelo_mercaderia_0 with modelo_mercaderia_1\n",
    "            if 'modelo_mercaderia_1' in df_costs.columns:\n",
    "                df_costs['modelo_mercaderia_0'] = df_costs['modelo_mercaderia_1'].combine_first(df_costs['modelo_mercaderia_0'])\n",
    "\n",
    "                # Drop 'modelo_mercaderia_1' column\n",
    "                df_costs.drop(columns=['modelo_mercaderia_1'], inplace=True)\n",
    "            \n",
    "            # Rename 'modelo_mercaderia_0' to 'modelo'\n",
    "            df_costs.rename(columns={'modelo_mercaderia_0': 'modelo'}, inplace=True)\n",
    "\n",
    "        # Drop unnecessary columns if they exist\n",
    "        columns_to_drop = [\n",
    "            'regimen_aduanero', 'refrendo', 'item', 'probable_importador', 'pais_de_origen', 'pais_de_procedencia', 'ciudad_embarque', 'via_de_transporte', 'aduana', 'partida_arancelaria', 'descripcion_arancelaria', 'descripción_arancelaria', \n",
    "            'descripcion_producto_comercial', 'estado_de_mercancia', 'unidad_de_medida', 'embarcador', \n",
    "            'empresa_de_transporte', 'pais_de_embarque', 'país_de_embarque', 'país_de_origen', 'agente_de_aduana', 'nave', 'conocimiento_de_embarque', 'peso_neto_kg', 'tipo_aforo', 'dirección_consignatario', 'caracteristicas', 'producto', 'marca', 'año_fabricación', 'código_tnan', 'contenedor',\n",
    "            'conocimiento_embarque', 'agencia_de_carga'\n",
    "        ]\n",
    "\n",
    "        # Only drop columns that exist in the dataframe\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df_costs.columns]\n",
    "        df_costs.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "\n",
    "        # Calculate 'us$_fob_unit' by dividing 'us$_fob' by 'cantidad' or 'unidades'\n",
    "        if 'us$_fob' in df_costs.columns:\n",
    "            if 'cantidad' in df_costs.columns:\n",
    "                df_costs['us$_fob_unit'] = df_costs.apply(\n",
    "                    lambda row: row['us$_fob'] / row['cantidad'] if pd.notna(row['cantidad']) and row['cantidad'] != 0 else np.nan, \n",
    "                    axis=1\n",
    "                )\n",
    "            elif 'unidades' in df_costs.columns:\n",
    "                df_costs['us$_fob_unit'] = df_costs.apply(\n",
    "                    lambda row: row['us$_fob'] / row['unidades'] if pd.notna(row['unidades']) and row['unidades'] != 0 else np.nan, \n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "        # Replace NaN values in 'us$_fob_unit' with the mean of the column\n",
    "        if 'us$_fob_unit' in df_costs.columns:\n",
    "            mean_fob_unit = df_costs['us$_fob_unit'].mean(skipna=True)\n",
    "            # Assigning back without inplace=True\n",
    "            df_costs['us$_fob_unit'] = df_costs['us$_fob_unit'].fillna(mean_fob_unit)\n",
    "\n",
    "        # Round 'us$_fob_unit' column\n",
    "        if 'us$_fob_unit' in df_costs.columns:\n",
    "            df_costs['us$_fob_unit'] = df_costs['us$_fob_unit'].round(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data cleaning: {e}\")\n",
    "    \n",
    "    return df_costs\n",
    "\n",
    "# Clean the data\n",
    "df_costs_clean = clean_data(df_costs.copy())\n",
    "df_costs_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_paths = [\n",
    "    '/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/wrangled/aerators_wrangled_2017.csv',\n",
    "    '/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/wrangled/aerators_wrangled_2018.csv',\n",
    "    '/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/wrangled/aerators_wrangled_2019.csv',\n",
    "    '/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/wrangled/aerators_wrangled_2020.csv',\n",
    "    '/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/wrangled/aerators_wrangled_2021.csv',\n",
    "    '/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/wrangled/aerators_wrangled_2022.csv',\n",
    "    '/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/processed/prospectos_importadores/concat/wrangled/aerators_wrangled_2023.csv'\n",
    "]\n",
    "\n",
    "# Read all the files into a list of DataFrames\n",
    "dataframes = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Convert 'fecha' column to datetime\n",
    "merged_df['fecha'] = pd.to_datetime(merged_df['fecha'])\n",
    "\n",
    "# Set 'fecha' as the index and sort chronologically\n",
    "merged_df = merged_df.set_index('fecha').sort_index()\n",
    "\n",
    "# Filter dataset where 'us$_fob_unit' is greater than $500\n",
    "filtered_df = merged_df[merged_df['us$_fob_unit'] > 500]\n",
    "\n",
    "# Calculate the mean of 'us$_fob_unit' in the filtered dataset\n",
    "mean_fob_unit = filtered_df['us$_fob_unit'].mean()\n",
    "\n",
    "# Extract the rows where 'us$_fob_unit' is greater than the mean to create a mini dataset\n",
    "mini_dataset = filtered_df[filtered_df['us$_fob_unit'] > mean_fob_unit]\n",
    "\n",
    "# Create a new dataset excluding rows greater than the mean of 'us$_fob_unit'\n",
    "df_aerator_data = filtered_df[filtered_df['us$_fob_unit'] <= mean_fob_unit]\n",
    "\n",
    "print(df_aerator_data.head())\n",
    "\n",
    "# Save df_aerator_data to a CSV file\n",
    "df_aerator_data.to_csv('/home/luisvinatea/Data/Gdrive/aquaculture/beraqua/reports/aerator_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
